
\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}
    \addtobeamertemplate{frametitle}{\vspace*{-0.1cm}}{\vspace*{-0.5cm}}

\setbeamercolor{frametitle}{fg=black,bg=white}

\usetheme{CambridgeUS}
\usepackage{ngerman}
\renewcommand{\figurename}{Figure}
\renewcommand{\tablename}{Tabel}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\begin{document}
\title[Deep Taylor Decomposition]{Explaining NonLinear Classification Decisions with Deep Taylor Decomposition\\
analysis}  
\author{Marcel Pommer}
\institute{LMU M\"unchen}
\date{\today} 

\begin{frame}
\titlepage
\end{frame} 

\begin{frame}
\frametitle[Explainability]{Explainability}
\begin{center}
\includegraphics[width=0.9\textwidth]{image/introduction_image}
\end{center}
\vspace{0.25cm}
Deep neural networks have great performance on a variety of problems \\
\textbf{but} how can we justify decisions made by complex deep architectures?\\
\end{frame} 


\begin{frame}
\frametitle[Table of Contents]{Table of Contents}
\vspace{0.4cm}
\tableofcontents
\end{frame} 

\AtBeginSection[]
{
\begin{frame}
\frametitle[Table of Contents]{Table of Contents}
\vspace{0.4cm}
\tableofcontents[currentsection]
\end{frame} 
}


\section[Introduction]{Introduction}
\begin{frame}
\frametitle{Introduction} 
\textbf{Deep neural networks revolutionized amongst others the field of}\\
\vspace{0.25cm}
       \begin{columns}[T]
          \column{0.65\textwidth}
             \centering
             \includegraphics[height=5cm, width=8cm]{image/NLP_chatbot}
           \column{0.35\textwidth}
			\begin{itemize}
			\item[--] Image recognition
			\item[--] Natural language processing
			\item[--] Human action recognition
			\item[--] Physics
			\item[--] Finance
			\item[--] ...
			\end{itemize}
         \end{columns} 
\vspace{0.5cm}
With one major drawback $\rightarrow$ \textbf{lack of transparency}
\end{frame}



\subsection[interpretable classifier]{Interpretable Classifier}
\begin{frame}
\frametitle{Interpretable Classifier} 
\vspace{0.5cm}
Explanation of non-linear classification in terms of the inputs\\
$\rightarrow$ A classifier should not only provide a result but also a reasoning

\begin{center}
\includegraphics[height=5cm, width=8cm]{image/cancer}
\end{center}

We do not only need to know if the patient has cancer but also where exactly it is located

\end{frame}



\subsection[General Idea]{General Idea}


\begin{frame}
\frametitle{General Idea} 
\vspace{0.35cm}
To accomplish the task of explainability we map relevance from the output to the input features

\vspace{0.25cm}

\begin{center}
\begin{figure}
\includegraphics[height=4.5cm, width=11cm]{image/fig._1_example}
\caption{NN detecting 0 while distrected by 5}
\end{figure}
\end{center}

\end{frame}




\section[Pixel-Wise Decomposition]{Pixel-Wise Decomposition}

\subsection[Mathematical Framework and Definitions]{Mathematical Framework and Definitions}

\begin{frame}
\frametitle{Mathematical Framework}
\vspace{0.5cm} 
In the context of image classification we define the following mathematical framework
\begin{itemize}
\item Positive valued function $f:\mathbb{R}^d\to \mathbb{R}^+$, where the output $f(x)$ defines either the probability that the object is present or the quantity of the object in question
\item[$\rightarrow$] $f(x)>0$ expresss the presence of the object  
\item Input $x \in \mathbb{R}^d$, composable in a set of pixel values $x=\{x_p\}$
\item Relevance score $R_p(x)$ indicating the relevance of each pixel
\item[$\rightarrow$] The relevance score can be displayed in a heatmap denoted by $R(x) = \{R_P(x)\}$
\end{itemize}

\begin{figure}
\includegraphics[height=2cm, width=2cm]{image/heatmap_example}
\end{figure}

\end{frame}




\begin{frame}
\frametitle{Definitions}
\begin{block}{Definition 1}
A heatmapping $R(x)$ is \underline{conservative} if the sum of assigned relevances in the pixel space corresponds to the total relevance detected by the model, that is
\begin{equation}
\forall x: f(x)=\sum_pR_p(x)
\end{equation}
\end{block}

\begin{block}{Definition 2}
A heatmapping $R(x)$ is  \underline{positive} if all values forming the heatmap are greater or equal to zero, that is:
\begin{equation}
\forall x,p: R_p(x) \geq 0
\end{equation}
\end{block}

\end{frame}


\begin{frame}
\frametitle{Definitions}
Further we test all algorithms for compliance with definition 1 and 2 why we introduce the definition of consistency which forth on is a measure of correctness of a technique\\
%\vspace{0.1cm}


\begin{block}{Definition 3}
A heatmapping $R(x)$ is  \underline{consistent} if it is  \textit{conservative} and  \textit{positive}. That is, it is consistent if it complies with Definitions 1 and 2.
\end{block}


\vspace{0.5cm}
However consistency is not a measure of quality which can be seen easily on the following example which complies with definition 3
\begin{equation*}
\forall p: R_p(x) =\frac{1}{d} \dot f(x) ,
\end{equation*}
where d denotes the number of pixels
\end{frame}



\subsection[Methodology]{Methodology}

\begin{frame}
\frametitle{Taylor Expansion}
\vspace{0.35cm}
First order taylor expansion at root point $\tilde{x}$
\begin{flalign*}
 f(x) & =f(\tilde{x}) + \left( \frac{\partial f}{\partial x}\Big|_{x=\tilde{x}}\right)^T \cdot (x-\tilde{x}) + \epsilon\\
       & = 0 + \sum_p \underbrace{\frac{\partial f}{\partial x_p}\Big|_{x=\tilde{x}} \cdot (x_p-\tilde{x_p})}_{R_p(x)} + \epsilon  
\end{flalign*}
The challenge of finding a root point
 \begin{columns}
          \begin{column}[T]{5.5cm}
             \begin{figure}
             \includegraphics[height=2.75cm, width=6cm]{image/fig._2_example_root_point}
             \end{figure}
            \end{column} 
            \begin{column}[T]{6.2cm}
			\begin{itemize}
			\item Potentially more than one root point
			\item Remove object but deviate as few as possible
			\item[$\rightarrow$] $\min_{\xi} ||\xi-x||^2 \text{ subject to } f(\xi)=0$
			\end{itemize}
	\end{column}
\end{columns} 


\end{frame}



\begin{frame}
\frametitle{Sensitivity Analysis}
Choose a root point at infinitisimally small distance from the actual point, i.e. $\xi = x- \delta \frac{\partial f}{\partial x}$\\
If we assume a locally constant function we get
\begin{flalign*}
 f(x) & =f(\xi) + \left( \frac{\partial f}{\partial x}\Big|_{x=\xi}\right)^T \cdot (x- (x- \delta \frac{\partial f}{\partial x})) + 0\\
       & = f(\xi) + \delta \left( \frac{\partial f}{\partial x}\right)^T  \frac{\partial f}{\partial x} + 0\\
       & = f(\xi) + \sum_p \underbrace{ \delta \left( \frac{\partial f}{\partial x}\right)^2}_{R_p} + 0
\end{flalign*}


\begin{itemize}
\item The heatmap is positive but not conservative
\item Measure local effect
\end{itemize}



\end{frame}



\begin{frame}
\frametitle{Deep Taylor Decomposition}
\vspace{0.25cm}
\begin{figure}
\includegraphics[height=2.75cm, width=11cm]{image/deep_taylor_decomposition_example}
 \end{figure}
We view each layer as a seperate function and write the Taylor decomposition of $\sum_j R_j \text{ at } \{x_i\}$ as
\begin{align*}
    \sum_j R_j &= \left( \frac{\partial (\sum_j R_j)}{\partial \{x_i\}}\Big|_{\{\tilde{x_i}\}}\right)^T \cdot (\{x_i\}-\{\tilde{x}\}) + \epsilon\\
    &= \sum_i \sum_j \frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}\}} \cdot (x_i-\tilde{x_i}) + \epsilon
\end{align*}


\end{frame}

\begin{frame}
\frametitle{Deep Taylor Decomposition}
\vspace{0.25cm}
\begin{figure}
\includegraphics[height=2.75cm, width=11cm]{image/deep_taylor_decomposition_example}
 \end{figure}

\begin{itemize}
\item  If each local Taylor decomposition is \textit{conservative} then the chain of equalitis is also \textit{conservative} (Layer-wise relevance conservation)
\item[$\rightarrow$] $R_f = ...= \sum_i R_i = ... = \sum_p R_p$
\item  If each local Taylor decomposition is \textit{positive} then the chain of equalitis is also \textit{positive} 
\item[$\rightarrow$] $R_f,...,\{R_i\},...,\{R_P\} \geq 0$
\item If each local Taylor decomposition is \textit{consistent} then the chain of equalitis is also \textit{consistent} 
\end{itemize}


\end{frame}

\section[One-Layer Networks]{Application to One-Layer Networks and Root Finding}



\begin{frame}
\frametitle{Setting}
\vspace{0.2cm}
Consider a simple detection-pooling one layer neural network with
 \begin{columns}
          \begin{column}[T]{5.5cm}
		\begin{align*}
		x_j &= \max(0, \sum_i x_iw_{ij} + b_j)\\
		x_k &= \sum_j x_j, \ b_j \leq 0, \forall j
		\end{align*}
            \end{column} 
            \begin{column}[T]{6.5cm}
			\begin{figure}
				\includegraphics[height=2.5cm, width=6cm]{image/one_layer_nn}
			\end{figure}
	\end{column}
\end{columns} 
\begin{enumerate}
\item $R_k = x_k$ and thus $R_k = \sum_j x_j$
\item  Chose a root point and redistribute $R_k$ on neurons $x_j$\\
$\rightarrow$ $R_j = \frac{\partial R_k}{\partial x_j}\Big|_{\{\tilde{x}_j\}} \cdot (x_j - \tilde{x}_j) $, with $\{x_j\}=0$
\item Since $\frac{\partial R}{\partial x_j}=1$ we obtain $R_j = x_j$
\item Apply Taylor decomposition another time and get\\
 $R_i = \sum_j\frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}_i\}^{(j)}} \cdot (x_i - \tilde{x}_i^{(j)})$
\end{enumerate}



\end{frame}


\begin{frame}
\frametitle{Derivation of Propagation Rules}
Given $R_j = \max(0, \sum_i x_iw_{ij} + b_j)$ and $b_j <0$ and a search direction $\{v_i\}^{(j)}$ in the input space such that
\vspace{-0.35cm}
\begin{equation}
\{\tilde{x}\}^{(j)} = \{x_i\} + t \{v_i \} ^{(j)} \Leftrightarrow t = \frac{\tilde{x_i}^{(j)} - x_i}{v_i^{(j)}}
\label{equ:search_direction}
\end{equation}
\vspace{-0.35cm}

If the data point itself is not a root point, i.e. $\sum_i x_i w_{ij} + b_j>0$ the nearest root along $\{v_i\}^{(j)}$ is given by the intersection of equation \eqref{equ:search_direction} and $\sum_i\tilde{ x_i}^{(j)} w_{ij} + b_j=0$ which can be resolved to 
\vspace{-0.25cm}
\begin{align*}
0 &= \sum_i\{x_i\}w_{ij} + b_j + \sum_i v_i^{(j)} t \\
x_i - \tilde{x_i}^{(j)} & = \frac{\sum_i x_i w_{ij} + b_j}{\sum_i v_i^{(j)} w_{ij}} v_i^{(j)}
\end{align*}

\end{frame}

\begin{frame}
\frametitle{Derivation of Propagation Rules}
\vspace{0.4cm}
Starting from the Taylor expansion we can plug in 
\begin{align*}
x_i - \tilde{x_i}^{(j)} = \frac{\sum_i x_i w_{ij} + b_j}{\sum_i v_i^{(j)} w_{ij}} v_i^{(j)}
\end{align*}
\vspace{-0.2cm}
To get
\vspace{-0.2cm}
\begin{align*}
R_i & = \sum_j \frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}_i^{(j)}} \dot (x_i - \tilde{x}_i^{(j)})= \sum_j w_{ij}\frac{\sum_i x_i w_{ij} + b_j}{\sum_i v_i^{(j)} w_{ij}} v_i^{(j)}\\
&= \sum_j w_{ij}\frac{v_i^{(j)} w_{ij}}{\sum_i v_i^{(j)} w_{ij}} R_j \numberthis \label{equ:relevance_model}
\end{align*}

The relevance propagation rule can now easily be calculated by defining
\begin{enumerate}
	\item Define a segment with search directio $\{v_i\}^{(j)}$
	\item The line lies inside the input domain and contains a root point
	\item Inject search direction in equation \eqref{equ:relevance_model}
\end{enumerate}
\end{frame}




\subsection[$w^2$-rule]{$w^2$-rule}


\begin{frame}
\frametitle{$w^2$-rule $\mathcal{X}=\mathbb{R}^d$}
Choose root point which is nearest in euclidean sense\\
\begin{enumerate}
	\item Search direction $\{v_i\}^{(j)} = w_{ij}$
	\item No domain restriction
	\item Inject search direction in equation \eqref{equ:relevance_model}
\end{enumerate}

\begin{block}{$w^2$-rule}
\begin{equation*}
R_i =\sum_j\frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}_i\}^{(j)}} \cdot (x_i - \tilde{x}_i^{(j)}) =  \sum_j\frac{w_{ij}^2}{\sum_i w_{ij}^2} R_j
\end{equation*}
\end{block}



\end{frame}



\begin{frame}
\frametitle{$w^2$-rule $\mathcal{X}=\mathbb{R}^d$}
\vspace{0.3cm}
\begin{block}{Proposition 1}
For all function $g \in G$, the deep Taylor decomposition with the $w^2$-rule is consistent.
\end{block}
\textbf{Proof}\\
\begin{enumerate}
\item \textit{Conservative}
\begin{align*}
\sum_i R_i &= \sum_i\Big(\sum_j \frac{w_{ij}^2}{\sum_iw_{ij}^2}R_j\Big)\\
   		&= \sum_j \frac{\sum_i w_{ij}^2}{\sum_iw_{ij}^2}R_j = \sum_j R_j = \sum_j x_j = f(x)
\end{align*}
\item \textit{Positive}
\begin{align*}
R_i = \sum_j \frac{w_{ij}^2}{\sum_i w_{ij}^2}R_j = \sum_j \underbrace{w_{ij}^2}_{>0}  \underbrace{\frac{1}{\sum_i w_{ij}^2}}_{>0}  \underbrace{R_j}_{\geq 0} \geq 0
\end{align*}
\end{enumerate}


\end{frame}




\subsection[$z^+$-rule]{$z^+$-rule}


\begin{frame}
\frametitle{$z^+$-rule $\mathcal{X}=\mathbb{R}_+^d$}
\vspace{0.25cm}
Search for a root point on the segment $(\{x_i 1_{w_{ij}<0}\},\{x_i\})\subset \mathbb{R}_+^d$\\
\begin{enumerate}
	\item Search direction $\{v_i\}^{(j)} = x_i - x_i 1_{w_{ij}<0} = x_i 1_{w_{ij}\geq0}$
	\item If $\{x_i\} \in \mathbb{R}_+^d$ so is the whole domain, further for $w_{ij}^-=\min(0,w_{ij})$
	\
	\begin{align*}
		R_j(\{x_i1_{w_{ij}<0}\}) = \max(0, \sum_i x_i 1_{w_{ij}<0} w_{ij} + b_j)\\
		= \max(0, \sum_i x_i  w_{ij}^- + b_j) = 0
	\end{align*}
	
	\item Inject search direction in equation \eqref{equ:relevance_model}
\end{enumerate}


\begin{block}{$z^+$-rule}
\begin{equation*}
R_i =  \sum_j \frac{x_i w_{ij}^+}{\sum_{i'} x_{i'} w_{i'j}^+} R_j
\end{equation*}
\end{block}

\end{frame}



\begin{frame}
\frametitle{$z^+$-rule $\mathcal{X}=\mathbb{R}^d_+$}
\vspace{-0.4cm}
\begin{block}{Proposition 2}
For all function $g \in G$ and data points $\{x_i\} \in \mathbb{R}_+^d$, the deep Taylor decomposition with the $z^+$-rule is consistent.
\end{block}
\vspace{0.5cm}
\textbf{Proof}\\
If $\sum_i x_i w_{ij}^+ >0$ the same proof as for the $w^2$-rule applies, if $\sum_i x_i w_{ij}^+ =0$ follows that $\forall i: x_i w_{ij} \leq0$ and 

\begin{align*}
 R_j = x_j = 0
\end{align*}
and there is no relevance to redistribute to the lower layers.


\end{frame}






\subsection[$z^b$-rule]{$z^b$-rule}


\begin{frame}
\frametitle{$z^b$-rule $\mathcal{X}=\mathcal{B}$}
\vspace{0.4cm}
Often we have a bounded input space $\mathcal{B} = \{\{x_i\} : \forall_{i=1}^d l_i\leq x_i \leq h_i\} $ and a segment $(\{l_i 1_{w_{ij}>0} + h_i 1_{w_{ij}<0}\},\{x_i\})\subset \mathcal{B}$\\
\begin{enumerate}
	\item Search direction $\{v_i\}^{(j)} = x_i - x_i 1_{w_{ij}<0} = x_i 1_{w_{ij}\geq0}$
	\item If $\{x_i\} \in \mathcal{B}$ so is the whole domain, further for $w_{ij}^-=\min(0,w_{ij})$ and $w_{ij}^+=\max(0,w_{ij})$
	\vspace{-0.25cm}
	\begin{align*}
		R_j(\{l_i 1_{w_{ij}>0} + h_i 1_{w_{ij}<0}\}) &= \max(0, \sum_i l_i 1_{w_{ij}>0} w_{ij} + h_i 1_{w_{ij}<0} w_{ij} + b_j)\\
		&=\max(0, \sum_i l_i w_{ij}^+ + h_i w_{ij}^- + b_j)= 0
	\end{align*}
	\vspace{-0.25cm}
	\item Inject search direction in equation \eqref{equ:relevance_model}
\end{enumerate}
\vspace{-0.25cm}

\begin{block}{$z^b$-rule}
\begin{equation*}
R_i =  \sum_j \frac{x_i w_{ij} - l_i w_{ij}^+ - h_i w_{ij}^-}{\sum_{i'} x_{i'} w_{i'j}  - l_i w_{i'j}^+ - h_i w_{i'j}^-} R_j
\end{equation*}
\end{block}

\end{frame}



\begin{frame}
\frametitle{$z^b$-rule $\mathcal{X}=\mathcal{B}$}
\vspace{-0.4cm}
\begin{block}{Proposition 3}
For all function $g \in G$ and data points $\{x_i\} \in \mathcal{B}$, the deep Taylor decomposition with the $z^b$-rule is consistent.
\end{block}
\vspace{0.5cm}
\textbf{Proof}\\
Since the proof is similar to the proofs of proposition 1 and 2 but lenghty I refer to the literature.

\end{frame}


\section[ausschlie{\ss}lich Allgemeinwohl]{Die Implementierung des PT ist unm\"oglich, wenn alle Agenten ausschlie{\ss}lich am Allgemeinwohl interessiert sind} 












\section[alternatives privates Motiv]{Diskussion: Ein Vorschlag zur Umformulierung des privaten Motivs}

\subsection{Ein alternatives privates Motiv}
\begin{frame}\frametitle{Ein alternatives privates Motiv}
\vspace{-1.2cm}
\textbf{Das alternative private Motiv:} Jeder Agent ist neben dem Allgemeinwohl daran interessiert, dass sich seine Empfehlung ex post als richtig erweist.
\vspace{0.4cm}
\begin{block}{Behauptung}
Der in Kapitel drei definierte Mechanismus implementiert das PT f\"ur jedes Profil von Pr\"aferenzen, das strikt wachsend im PT und dem oben definierten privaten Motiv ist.
\end{block}


\end{frame}





\begin{frame}\frametitle{Beweis}
\vspace{0.4cm}
Der Beweis erfolgt weitestgehend analog zum Beweis von Proposition 2. Im Folgenden die Unterschiede:
\begin{itemize}
\item Es gilt $\pi_1 \geq V(1)$, aber $\pi_{2,1}<1$. Trotzdem steht das private Motiv von Agent 1 dem \"offentlichen nicht entgegen.
\item [$\rightarrow$] Agent 1 maximiert $\pi_{2,1}<1$, indem er $\pi_1$ maximiert, damit w\"ahlt er $S$ so informativ wie m\"oglich.
\item Analog zum Beweis von Proposition 2 folgt damit $S_{NT}=\varnothing$.
\item Mit Hilfslemma 1 folgt ebenfalls, dass $S_c=\varnothing$, da ein Agent durch einen Wechsel von der Strategie ``c'', zur Strategie ``T'', $\pi_{2,i}$ von $\frac{1}{2}$ auf $p$ erh\"oht.
\item Da alle Agenten $i\notin S$ ausschlie{\ss}lich daran interessiert sind, dass sich ihre Empfehlung ex post als richtig erweist, spielen sie ``T''.
\item [$\rightarrow$] Damit folgt, dass Agent 1 $S=N\setminus \{1\}$ w\"ahlt und alle Agenten in $S$ spielen ``T''. Agent 1 folgt der Mehrheit und im Fall eines Unentschieden folgt er seinem eigenen Signal.
\end{itemize}
\end{frame}









\section{Literatur}
\begin{frame}
\frametitle{Literatur}
\footnotesize{
\begin{thebibliography}{99}
\bibitem[wik]{p1} [GR98] Jacob Glazer and Ariel Rubinstein. Motives and implementation: On the design of mechanism to elicit opinions. Journal of economic Theory, 79(2):157-173, 1998.
\end{thebibliography}
}
\end{frame}






\end{document}