% This file was created with Citavi 6.12.0.0

@book{Eaton.1998,
 abstract = {{\textquotedbl}The glamorous yet tragic story of the Titanic, her sinking with the loss of more than 1,500 lives, and the mysterious aftermath, still holds endless fascination over 80 years later. This comprehensive chronicle of the entire saga, much lauded by the experts whenit was first published, has been revised and expanded and now brings the strory right up to date.{\textquotedbl}--Back cover.},
 author = {Eaton, John P. and Haas, Charles A. and Maxtone-Graham, John},
 year = {1998},
 title = {Titanic: Triumph and tragedy},
 address = {Nr. Yeovil, Somerset},
 edition = {2nd ed.},
 publisher = {{Patrick Stephens}},
 isbn = {185260493X}
}


@article{Russakovsky.2015,
 author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
 year = {2015},
 title = {ImageNet Large Scale Visual Recognition Challenge},
 pages = {211--252},
 volume = {115},
 number = {3},
 issn = {0920-5691},
 journal = {International Journal of Computer Vision},
 doi = {10.1007/s11263-015-0816-y}
}


@article{MaximilianAlber.2019b,
 author = {{Maximilian Alber} and {Sebastian Lapuschkin} and {Philipp Seegerer} and {Miriam H{\"a}gele} and {Kristof T. Sch{\"u}tt} and {Gr{\'e}goire Montavon} and {Wojciech Samek} and {Klaus-Robert M{\"u}ller} and {Sven D{\"a}hne} and {Pieter-Jan Kindermans}},
 year = {2019},
 title = {iNNvestigate Neural Networks!},
 url = {http://jmlr.org/papers/v20/18-540.html},
 pages = {1--8},
 volume = {20},
 number = {93},
 journal = {Journal of Machine Learning Research}
}


@article{Montavon.2017b,
 abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
 author = {Montavon, Gr{\'e}goire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
 year = {2017},
 title = {Explaining NonLinear Classification Decisions with Deep Taylor  Decomposition},
 url = {http://arxiv.org/pdf/1512.02479v1},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 pages = {211--222},
 volume = {65},
 number = {2},
 issn = {00313203},
 journal = {Pattern Recognition},
 doi = {10.1016/j.patcog.2016.11.008}
}


