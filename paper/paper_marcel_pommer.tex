% SIAM Article Template
% remove review to surprass the wtermark at the end of every page
%\documentclass[review,hidelinks,onefignum,onetabnum]{siamart220329}

\documentclass[review,hidelinks,onefignum,onetabnum]{siamart220329}
% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}



% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={Explaining NonLinear Classification Ddecisions with Deep Taylor Decompositions},
  pdfauthor={Marcel Pommer}
}
\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

%\externaldocument[][nocite]{ex_supplement}

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

\begin{document}

\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{4pt}
\setlength{\abovedisplayshortskip}{4pt}
\setlength{\belowdisplayshortskip}{4pt}

\maketitle

% REQUIRED
\begin{abstract}
During the last decade Deep Neural Networks (DNNs) as well as other sophisticated machine learning models gained substantially on relevance, due to so far unreached performance in a variety of topics like image recognition, finance or natural language processing, to name only a few. Despite their great performance, those, mostly non linear models, lack of one important aspect, the explainability of the results. Montavon et al. introduce in their paper \textit{Explaining NonLinear Classification Decisions with Deep Taylor Decompositions} \cite{Montavon.2017b} a new technique, the deep taylor decomposition, to map the relevance of the output on the input features, i.e. quantify the influence of each input variable on the output. They demonstrate the results on two image recognition data sets, the MNIST \cite{lecun2010mnist} and the ILSVRC \cite{Russakovsky.2015}, creating heatmaps to display the relevance of each single pixel. I recreate the procedure in Python and apply the deep taylor decomposition to the titanic dataset \cite{Eaton.1998}.
\end{abstract}

% REQUIRED
\begin{keywords}
Explainability, Deep Neural Networks, Image Recognition
\end{keywords}

% REQUIRED
\begin{MSCcodes}
62H35, 93B15
\end{MSCcodes}

\section{Introduction}
The raise of machine learning, combined with steadily growing computational power revolutionized many so far hard to grasp tasks like image recognition and are highly used in countless areas like self driving cars and diagnoses of deseases. Those new techniques became quite famous during the last decade due to their overperformance in nearly every field, however their complexity makes them mathematicaly hard to understand and explain, leading to one of their major drawbacks, missing explainability. The paper \textit{Explaining NonLinear Classification Decisions with Deep Taylor Decompositions} \cite{Montavon.2017b} by Montavon et al. tries to tackle this problem by extending the explainability of deep neural networks using taylor expansion, resulting in a mapping of non negative relevance from the output to each input features. The authors apply their technique to a variety of examples from the MNIST \cite{lecun2010mnist} dataset as well as the ILSVRC \cite{Russakovsky.2015} dataset. figure \ref{fig.1} shows a tractable example, in which a neural network detects a `0` while distracted by a `5`. We denote the neurons with $x_i$ and the respective contributions with $R_i$, resulting in a graphical representation, i.e. a heatmap, indicating which pixels contribut with which intensity to the decision of the neural network.
\begin{figure}[ht]
	\vspace{-0.25cm}
	\centering
      \includegraphics[width=0.5\textwidth]{images/fig._1_example}
	\caption{Example: Detecting 0 with distrecting numbers with a neural network}
	\label{fig.1}
\end{figure}
\vspace{-0.35cm}
Montavon et al. focus on image recognition, but highlight, that the procedure can be broadcasted to any input space and feature set. In my analysis of the application of the algorithm I will focus on the titanic \cite{Eaton.1998} dataset which is very tractable and easy to understand and interpret.

% The outline is not required, but we show an example here.
%The paper is organized as follows. The main results are in \ref{sec:main}, the application on a simple neural network and algorithms in  section \ref{sec:alg}, examples on the MNIST data set and experiments on the titanic dataset in section \ref{sec:exp}, and the conclusion follows in section \ref{sec:conclusions}.

\section{Main results}
\label{sec:main}
The main section summarizes the general idea and presents the definitions and theorems while focusing on image recognition and following the metodology of Montavon et al. In the context of image classification, we define a d-dimensional input space $x \in\mathbb{R}^d$, where the image pixels ($p$) can be represented as $x=\{x_p\}$. The function $f(x):\mathbb{R}^d \to \mathbb{R}^+$ quantifies either the probability of an object in the picture or the quantity of the object in question. The aim of the deep taylor decomposition is to assign a relevance score $R_p(x)$ to each pixel $p$ in the input space. The relevance score quantifies the explanatory power of each pixel, i.e. the higher the relevance score the more important was the pixel for the classification. If the result is plotted in an image or to say heatmap as displayed in figure 1 the pixels which led to the classification decision are highligthed. In practice some conditions can help to further define and understand the relevance score. In the context of heatmaps, but also in other cases, the authors state three definitions. 

\begin{definition}[conservative]
\label{thm:conservative}
A heatmapping $R(x)$ is \underline{conservative} if the sum
of assigned relevances in the pixel space corresponds to the
total relevance detected by the model, that is
  \vspace{-0.1cm}
  \begin{displaymath}
    \forall x: f(x) = \sum_p R_p(x)
  \end{displaymath}
\end{definition}
  \vspace{-0.1cm}
In other words, the sum of the relevance of all pixels should align with the output. Definition \ref{thm:conservative} ensures that all relevance detected by the model can be explained by the input variables. 

\begin{definition}[positive]
\label{thm:positive}
A heatmapping $R(x)$ is \underline{positive} if all values
forming the heatmap are greater or equal to zero, that is:
  \vspace{-0.1cm}
  \begin{displaymath}
    \forall x, p: R_p(x) \geq 0
  \end{displaymath}
\end{definition}
\vspace{-0.15cm}
This property ensures, that relevance cannot be negative in the sense that two pixels cancel each other out. Since definition \ref{thm:conservative} and definition \ref{thm:positive} are of essence for the evaluation of models we further define:
\begin{definition}[consistent]
\label{thm:consistent}
A heatmapping $R(x)$ is \underline{consistent} if it is \textit{conservative} \underline{and} \textit{positive}.
\end{definition}
We will use definition \ref{thm:consistent} to evaluate heatmaps, however consistency is not necessarily a measure of quality, which can be seen in the following example of a uniform relevance over all $d$ pixels where $\forall p: R_p(x) = \frac{1}{d} f(x)$. Although, the heatmap will comply with definition \ref{thm:consistent} it will result in an all black/grey image providing no further information on the relation between input and output.


\section{Algorithms}
\label{sec:alg}
The deep taylor decomposition is based on the first order taylor expansion at a root point $\tilde{x}$, such that $f(\tilde{x})=0$:
\begin{align}
    f(x)=f(\tilde{x}) + \left( \frac{\partial f}{\partial x}\Big|_{x=\tilde{x}}\right)^T \cdot (x-\tilde{x}) + \epsilon
    = 0 + \sum_p \frac{\partial f}{\partial x_p}\Big|_{x=\tilde{x}} \cdot (x_p-\tilde{x_p}) + \epsilon  
 \label{eq:taylorDecomp},
\end{align}
where the sum over all pixels derivative is defined as the redistributed relevance:
\begin{align*}
    R(x)=\frac{\partial f}{\partial x}\Big|_{x=\tilde{x}} \odot (x-\tilde{x}),
\end{align*}
and $\odot$ is defined as element wise multiplication. The finding of the root point is a great challenge and far from obvious. In figure 2 we can see an image, where the root point is simply a variant of the picture where the building is blurrred.
\begin{figure}[ht]
	\centering
      \includegraphics[width=0.5\textwidth]{images/fig._2_example_root_point}
	\caption{Example: Root point in an image.}
	\label{fig.2}
	\vspace{-0.5cm}
\end{figure}
Since an image possibly can have more than one root point the choice is crucial and a good root point deviates from the original point $x$ as few as possible, i.e. minimizing the obective:
\begin{align*}
    \min_{\xi} ||\xi-x||^2 \text{ subject to } f(\xi)=0 \text{     and   }\xi \in \mathbb{X},
    \vspace{-0.1cm}
\end{align*}
where $\mathbb{X}$ is the input domain.\\

Next we focus on the deep taylor decomposition, where we consider the mapping of neurons in one layer to each neuron in the next layer, assuming a relation explainable by some relevance function $R_j(\{x_i\})$. If this assumption holds and we further can identify a root point $\{\tilde{x_i}\}$ such that $R_j(\{\tilde{x_i}\})=0$ we can write equation \ref{eq:taylorDecomp}:
\begin{align*}
    \sum_j R_j= \left( \frac{\partial (\sum_j R_j)}{\partial \{x_i\}}\Big|_{\{\tilde{x_i}\}}\right)^T \cdot (\{x_i\}-\{\tilde{x}\}) + \epsilon
    = \sum_i \sum_j \frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}\}} \cdot (x_i-\tilde{x_i}) + \epsilon
\end{align*}
If definition \ref{thm:consistent} holds, the relevances is guaranteed to be conserved in each layer, i.e. $R_f=...=\sum_j R_j=...=\sum_p R_p$ and $R_f,...,\{R_j\},...\{R_p\} \geq 0$.\\

Further I will present two different approaches, the $\omega^2$-rule and the $z$-rule. As a starting point we consider a simple detection pooling neural network with a rectified linear activation function, i.e.
\begin{align*}
x_j = \max(0, \sum_i x_iw_{ij} + b_j),\ x_k = \sum_j x_j,
\end{align*}
where $\{x_i\}$ is a d-dimensional input and $\theta = \{w_{ij},b_j\}$ are weights and bias. To guarantee the exististence of a root point in the origin we restrict $b_j\leq0$. The relevance of the top layer is due to the pooling $R_k=\sum_j x_j$ and can be redistributed to the next layer according to the taylor decomposition
\begin{align*}
R_j = \frac{\partial R_k}{\partial x_j}\Big|_{\{\tilde{x}_j\}} \cdot (x_j - \tilde{x}_j) 
\end{align*}
If either $\sum_j\tilde{x}_j=0$ and $\forall j: \tilde{x}_j \geq0$ we need to chose $\{\tilde{x}_j\}$ resulting in $R_j = x_j$, since $\frac{\partial R_k}{\partial x_j} = 1$. Redistributing the relevance to the next layer using the taylor decomposition leads to 
\begin{align}
R_i = \sum_j\frac{\partial R_j}{\partial x_i}\Big|_{\{\tilde{x}_i\}^{(j)}} \cdot (x_i - \tilde{x}_i^{(j)})
\label{equ:propagationRule}
\end{align}
which is the starting point for the further analysis.


\subsection{Unconstrained Input Space and $\omega^2$-Rule}
\label{sec:wRule}
If we consider an unconstrained input space one can chose the nearest root point in the Euclidean sense. Considering the rectified linear activation the intersection of the equation $\sum_i \tilde{x}_i^{(j)} w_{i,j}+b_j$ and the line of maximum descent defined by the derivative $\{\tilde{x}_i\}^{(j)}=\{x_i\} +tw_j$, where $w_i$ ist the weight vector and $t \in \mathbb{R}$ define the nearest root point which is then given by $\{\tilde{x}_i\}^{(j)} = \{x_i - \frac{w_{ij}}{\sum_i w_{ij}^2}(\sum_i x_i w_{ij}+b_j)\}$. Pluggin in equation \ref{equ:propagationRule} leads to
\begin{align*}
R_i = \sum_j\frac{w_{ij}^2}{\sum_i w_{ij}^2} R_j
\end{align*}

\begin{proposition}[$w^2$-Rule consistency]
\label{prop:wconsistency}
$\forall g \in G$, the deep Taylor decomposition with the $w^2$-rule is consistent in the sense of definition 3.
\end{proposition}

\subsection{Constrained Input Space and the z-Rules}
\label{sec:zRule}
Since in many cases the input domain is restricted the authors present a rule for bounded input spaces as well. Montavon et al. consider $\mathcal{X} = \mathbb{R}_x^d$ and $\mathcal{B} = \{\{x_i\} : \forall_{i=1}^d l_i\leq x_i \leq h_i\}$, where $l_i$ and $h_i$ are the respective lower and higher bounds for each input feature. I will only cover the first case since logic and results are quiet similar and the domain corresponds to the rectified linear activation. Hence we already know of the existence of one root at the origin we search on the segment $(\{x_i 1_{w_{ij}<0}\},\{x_i\})$ and thus the direction of the segment is given by $v_i^{(j)} = x_i - x_i 1_{w_{ij}<0} = x_i 1_{w_{ij}\geq0}$. If we follow the same logic as in section \ref{sec:wRule} but adjust the line of maximum descent to $\{\tilde{x}_i\}^{(j)}=\{x_i\} +tx_i 1_{w_{ij}\geq0}$ we get the following relevance propagation rule
\begin{align*}
R_i = \sum_j\frac{x_i 1_{w_{ij}\geq0}}{\sum_{i´} x_{i´} 1_{w_{i´j}\geq0}} R_j
\end{align*}
\begin{proposition}[$z$-Rule consistency]\label{prop:zconsistency}
$\forall g \in G$, the deep Taylor decomposition with the z-rule is consistent in the sense of Definition 3.
\end{proposition}

\subsection{Deep Neural Networks}
\label{sec:deepNeuralNetworks}
Since many neural networks use complex deep architectures the authors further show a tractable way for the mapping of relevance from higher to the lower layers if the mapping is not explicit and introduce the concept of relevance models. The Min-Max and the Training-Free relevance model are introduced in the paper, however I will only cover the first. It is defined as 
\begin{align*}
y_j = \max(0, \sum_i x_i v_{ij} + a_j), \text{  } \hat{R}_k = \sum_j y_j,
\end{align*}
where $a_j = \min(0,\sum_lR_lv_{lj} + d_j)$ is a negative bias where the sum runs over the detection neurons from the upper layer and $R_l$ are the corresponding relevances. After estimation of the paramters $\{v_{ij}, v_{lj}, d_j\}$ by minimizing 
\begin{equation}
min\langle(\hat{R}_k - R_k)^2\rangle 
\label{min:relevanceModel}
\end{equation}
, where $R_k$ and $\hat{R}_k$ are the true and predicted relevances, we end up with the same problem as in section \ref{sec:main}. Due to the similar structure we can apply the same computations and compute $R_j = y_j$ and $R_i = \sum_j \frac{q_{ij}}{\sum_{i´}q_{i´j}} R_j$, where $q_{ij} = \{w_{ij}^2, x_i w_{ij}1_{w_{ij}>0}\}$ for the $w^2$-rule and $z^+$-rule, respectively. In contrast to the problem before the resulting heatmap is only approximately conservative, due to the possible errors during the minimization of  \ref{min:relevanceModel}. 



\section{Experiment}
\label{sec:exp}
As mentioned in the beginning the authors focus on image recognition and present examples from the MNIST \cite{lecun2010mnist} and the ILSVRC \cite{Russakovsky.2015} data sets. I will only present one example from the MNIST \cite{lecun2010mnist} data set and conclude with a self made example from the titanic \cite{Eaton.1998} data set.  

\subsection{MNIST Example}
Figure \ref{fig.1} shows how the proposed algorithms can detect pixels with a high relevance for detecting a `0` next to a distracting number. Montavon et all show several examples of other numbers as in figure \ref{fig.3} and compare several methods. It can be seen that the heatmap clearly identifies the important pixels, i.e. correctly assigns relevance to areas where the digit is located. Further analysis shows, that especially the $w^2$- and $z$-rule are conservative and positive an thus satisfy the condition for definition \ref{thm:consistent}.
\begin{figure}[ht]
	\centering
      \includegraphics[width=0.75\textwidth]{images/fig._3_MNIST}
	\caption{Example: Detecting 0 with distrecting other numbers present.}
	\label{fig.3}
\end{figure}
%\vspace{-0.25cm}

\subsection{Titanic Example}
Neural networks are not only used for image recognition but for a variety of problems like the prominent titanic survival classification which I chose for my personal analysis. I build a simple neural network with one hidden layer and 10 neurons resulting in a accuracy of 80 \%. Table 1 summarizes the results and clearly shows, that features like sex and age have greater influence on the survival probability than features like the pasenger ID or the port of embarkation.\footnote{Code can be found on: \url{https://github.com/mpommer/Deep-Taylor-Decomposition-Python}}

\begin{table}[h!]
\centering
\caption{Relevance of titanic input features}
\label{table_1}
\begin{tabular}{ |c||c|c|c|c| }

 \hline 
  feature & sex& age & parch & passenger id\\
 \hline
 relevance in \% & 10 & 10 & 10 & 10\\
 \hline
\end{tabular}

\end{table}

\section{Conclusion}
\label{sec:conclusions}
During the last years the importance of explainability of complex machine learning models became more and more present, due to either simple questions of understanding or even legal issues in for example pricing of insurances. Montavon et al. present in their paper \textit{Explaining NonLinear Classification Decisions with Deep Taylor Decompositions} \cite{Montavon.2017b} practical algorithms which can help to tackle the problem of interpreteability for a wide range of deep learning models. The authors furthermore substantiate their theory with examples on two very famous image recognition data sets, namely the MNIST \cite{lecun2010mnist} and the ILSVRC \cite{Russakovsky.2015} data sets. I myself could also reproduce very reliable results on the titanic \cite{Eaton.1998} data set.


\bibliographystyle{siamplain}
\bibliography{literature_seminar}
\end{document}
